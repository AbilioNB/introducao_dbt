# IntroduÃ§Ã£o ao DBT

# ğŸ§± 1. Conceitos de uma Stack Moderna para Pipeline de Dados

---

## ğŸ” Pipeline de Dados

Um pipeline de dados Ã© uma sequÃªncia de processos automatizados que captura, transforma e carrega dados de diversas fontes para um destino, normalmente com o objetivo de anÃ¡lise, visualizaÃ§Ã£o ou tomada de decisÃ£o.

### Etapas comuns:
- **IngestÃ£o:** captura de dados (de APIs, bancos, arquivos etc.)
- **TransformaÃ§Ã£o:** limpeza, enriquecimento e reestruturaÃ§Ã£o dos dados
- **Armazenamento e consulta:** envio para bancos de dados, data warehouses ou data lakes
- **Consumo:** dashboards, modelos de ML, relatÃ³rios

---

## ğŸ”„ ELT vs ETL

### ETL (Extract, Transform, Load):
- **ExtraÃ§Ã£o:** obtÃ©m dados da fonte
- **TransformaÃ§Ã£o:** limpa e organiza antes de armazenar
- **Carga:** envia os dados prontos ao destino

ğŸ§  **Usado quando:** hÃ¡ necessidade de transformar os dados antes de armazenar.

---

### ELT (Extract, Load, Transform):
- **ExtraÃ§Ã£o:** obtÃ©m dados da fonte
- **Carga:** carrega os dados brutos no destino
- **TransformaÃ§Ã£o:** transforma os dados **dentro do banco** (ex: via SQL)

ğŸ§  **Usado quando:** o banco de destino Ã© potente e permite transformaÃ§Ã£o rÃ¡pida (ex: Snowflake, BigQuery, DuckDB)

---

## âš™ï¸ Ferramentas de InserÃ§Ã£o

Essas ferramentas automatizam a **ingestÃ£o** de dados, conectando-se a APIs, arquivos ou bancos de dados.

### Exemplos:
- **Airbyte** â€“ cÃ³digo aberto, permite conectar diversas fontes e destinos
- **Fivetran** â€“ soluÃ§Ã£o paga com foco em ELT
- **Singer** â€“ especificaÃ§Ã£o para ingestÃ£o de dados via arquivos "tap" e "target"
- **Python personalizado** â€“ scripts para ingestÃ£o sob medida

---

## ğŸ”§ Ferramentas de Pipelines de Dados

SÃ£o usadas para **orquestrar** as etapas do pipeline (agendamento, monitoramento, dependÃªncias).

### Exemplos:
- **Prefect** â€“ pipelines com foco em simplicidade e observabilidade
- **Dagster** â€“ design modular, com foco em desenvolvimento e validaÃ§Ã£o
- **Airflow** â€“ robusto e altamente configurÃ¡vel, ideal para ambientes complexos
- **Luigi** â€“ bom para pipelines locais e dependÃªncias simples
- **Make / bash / crontab** â€“ soluÃ§Ãµes caseiras e simples para fluxos pequenos

---

## ğŸ§± Banco de Dados Colunares

Diferente dos bancos tradicionais (linha a linha), os bancos colunares armazenam os dados **por coluna**, o que permite otimizaÃ§Ãµes para consultas analÃ­ticas.

### ğŸ†š Banco de Dados Colunares vs Relacionais

| CaracterÃ­stica      | Relacional (row-based) | Colunar (columnar)       |
|---------------------|------------------------|--------------------------|
| Armazenamento       | Linha por linha        | Coluna por coluna        |
| Otimizado para      | Escrita, OLTP          | Leitura, OLAP            |
| Exemplos            | PostgreSQL, MySQL      | DuckDB, ClickHouse, BigQuery |

---

## ğŸ¦† DuckDB

DuckDB Ã© um banco de dados analÃ­tico **colunar**, projetado para rodar **localmente**, dentro do seu processo Python (sem servidor).

### Vantagens:
- Extremamente leve (sem necessidade de servidor)
- Suporte nativo a formatos como Parquet e CSV
- IntegraÃ§Ã£o direta com pandas, Arrow e NumPy
- Alta performance em queries analÃ­ticas
- Ideal para prototipaÃ§Ã£o e ambientes de desenvolvimento

---

## ğŸ§ª Formas de Uso do DuckDB

- Como banco **embutido** em scripts e notebooks
- Com arquivos locais `.csv`, `.parquet`, `.json`
- Para consultar **DataFrames diretamente** (sem gravar no disco)
- Como camada intermediÃ¡ria de anÃ¡lise antes de enviar para um warehouse

### Exemplos:
```python
import duckdb
import pandas as pd

df = pd.read_csv("dados.csv")
duckdb.query("SELECT * FROM df WHERE valor > 100").to_df()
```




# ğŸ§© 2. Utilizando o DBT (Data Build Tool)

---

## ğŸ“Œ O que Ã© o DBT?

DBT (Data Build Tool) Ã© uma ferramenta que permite aplicar princÃ­pios de engenharia de software Ã  transformaÃ§Ã£o de dados no ambiente de dados analÃ­ticos.

ğŸ¯ **PropÃ³sito:** Transformar dados jÃ¡ carregados em um banco de dados (ex: DuckDB, BigQuery, Redshift) usando **SQL modular, versionado e testado**.

---

## âš™ï¸ Como funciona o DBT?

1. VocÃª cria **models** em arquivos SQL, organizados por camadas (ex.: `staging/`, `intermediate/`, `marts/`).
2. DBT converte esses arquivos em SQL executÃ¡vel no warehouse, respeitando dependÃªncias (via `ref()`).
3. Gera documentaÃ§Ã£o visual, realiza testes de qualidade, cria lineage (linhagem) e suporta versionamento com Git.
4. NÃ£o Ã© ferramenta de ingestÃ£o ou visualizaÃ§Ã£o â€” foca na transformaÃ§Ã£o (`T` de ELT).

ğŸ¯ VocÃª **nÃ£o move dados com o DBT**, mas transforma **dados jÃ¡ existentes no banco**.

---

## â˜ï¸ Dbt Cloud vs ğŸ–¥ï¸ Dbt Core

| Item             | DBT Cloud                        | DBT Core (open source)         |
|------------------|----------------------------------|-------------------------------|
| Interface        | Web, com UI e agendador          | Linha de comando (CLI)        |
| Infraestrutura   | DBT executa na nuvem             | VocÃª executa localmente       |
| PreÃ§o            | Gratuito (limitado) / Pago       | Gratuito                      |
| Recursos extras  | Scheduler, logs, alertas, UI     | Totalmente manual             |
| Ideal para       | Equipes e ambientes gerenciados  | Aprendizado e pequenos projetos |

---

## ğŸ§± OrganizaÃ§Ã£o das camadas: raw â†’ staging â†’ intermediate â†’ serving

O DBT incentiva uma estrutura em **camadas**, baseada em boas prÃ¡ticas de modelagem de dados analÃ­ticos:

### 1. **Raw**
- Dados crus, diretamente da fonte
- Sem transformaÃ§Ãµes

1. **Staging** (`models/staging/`)  
   - Prepara os dados "atomizados" vindos das fontes (raw), usando **sources**.  
   - Renomeia colunas, padroniza tipos e formatos.  
   - Serve de Ãºnica referÃªncia Ã s tabelas de origem, reduzindo acoplamento.  


2. **Intermediate** (`models/intermediate/`)  
   - Agrega lÃ³gica de negÃ³cio intermediÃ¡ria.  
   - Organiza os dados por grupo de negÃ³cio (ex.: `finance/`, `marketing/`).  
   - Geralmente possui verbos na nomenclatura (ex.: `int_payments_pivoted_to_orders.sql`)  


3. **Marts** (ou **Marts**) (`models/marts/`)  
   - Dados finais preparados para anÃ¡lise e relatÃ³rios.  
   - Combina dados processados para formar as entidades Ãºteis para o negÃ³cio. 

ğŸ“Œ Essa organizaÃ§Ã£o modulariza o projeto e facilita a manutenÃ§Ã£o e entendimento do fluxo de dados.

AlÃ©m dessas, tambÃ©m podem existir:
- **raw**â€” Dados integralmente copiados de outras fontes de dados, onde nÃ£o Ã© aplicada nenhuma alteraÃ§Ã£o.
- **utilities** â€” modelos utilitÃ¡rios reutilizÃ¡veis (ex.: tabela de datas), nÃ£o contÃªm dados de negÃ³cios propriamente ditos.  

---

## ğŸ”§ O que o DBT faz?

âœ… Transforma dados com SQL  
âœ… Controla dependÃªncias entre modelos  
âœ… Roda transformaÃ§Ãµes com versionamento (Git)  
âœ… Valida com **testes automatizados**  
âœ… Documenta todo o projeto com **interface navegÃ¡vel**  
âœ… Gera **lineage (linhagem)** de dados para rastrear o fluxo

---

## ğŸš« O que o DBT **nÃ£o faz**

â›” NÃ£o extrai dados de fontes (nÃ£o faz ingestÃ£o)  
â›” NÃ£o carrega dados para o banco (nÃ£o Ã© ETL completo)  
â›” NÃ£o cria dashboards ou relatÃ³rios  
â›” NÃ£o armazena dados em si  
â›” NÃ£o Ã© ferramenta de Machine Learning

---

## ğŸ“š DocumentaÃ§Ã£o com DBT

Com um Ãºnico comando (`dbt docs generate`), o DBT cria uma documentaÃ§Ã£o HTML com:

- DefiniÃ§Ã£o de cada modelo
- DescriÃ§Ã£o das colunas
- RelaÃ§Ãµes entre tabelas (lineage)
- Testes aplicados
- Seeds, snapshots, macros

ğŸ“ Ã‰ possÃ­vel acessar via navegador (`dbt docs serve`) ou publicar no DBT Cloud.

---

## âœ… Testes no DBT

DBT possui testes **integrados e customizados**, aplicados diretamente no SQL.

### Testes integrados (genÃ©ricos):
```yml
models:
  - name: vendas
    columns:
      - name: id_venda
        tests:
          - unique
          - not_null
```

# ğŸš€ 2.1. DBT AvanÃ§ado

Neste tÃ³pico, exploramos funcionalidades mais avanÃ§adas do DBT que tornam os projetos mais dinÃ¢micos, reutilizÃ¡veis e auditÃ¡veis: **macros**, **snapshots** e **seeds**.

---

## ğŸ§  2.1.1. Macros

### O que sÃ£o?
Macros sÃ£o **funÃ§Ãµes reutilizÃ¡veis** escritas em Jinja (um template engine para Python), usadas para gerar SQL dinamicamente dentro do DBT.

ğŸ” **Servem para:**
- Reduzir repetiÃ§Ã£o de cÃ³digo SQL
- Criar lÃ³gica condicional ou parametrizada
- Padronizar transformaÃ§Ãµes

---

### Exemplo bÃ¡sico de macro

ğŸ“ Arquivo: `macros/formatar_datas.sql`

```sql
{% macro formatar_data(coluna) %}
    date_trunc('day', {{ coluna }})
{% endmacro %}
```


```sql
SELECT
    {{ formatar_data("data_pedido") }} AS data_formatada
FROM {{ ref('stg_pedidos') }}
```

## ğŸ§¬ 2.1.2 Snapshots

### O que sÃ£o?

Snapshots sÃ£o usados para capturar mudanÃ§as histÃ³ricas em dados dimensionais, como SCDs (Slowly Changing Dimensions). Eles permitem rastrear como um registro mudou ao longo do tempo.

ğŸ” **Servem para:**
VocÃª tem uma tabela de clientes e deseja registrar cada vez que o endereÃ§o de um cliente mudar, mantendo o histÃ³rico.

### Exemplo bÃ¡sico de snapshot
ğŸ“ Arquivo: snapshots/clientes_snapshot.sql

```sql
{% snapshot clientes_snapshot %}

{{
  config(
    target_schema='snapshots',
    unique_key='id_cliente',
    strategy='check',
    check_cols=['nome', 'email', 'endereco']
  )
}}

SELECT * FROM {{ source('vendas', 'clientes') }}

{% endsnapshot %}

```

ğŸ“ dbt_project.yml (configuraÃ§Ã£o):

```yaml
snapshots:
  my_dbt_project:
    +target_schema: snapshots
    +strategy: check
```

## ğŸŒ± 2.1.3  Seeds

### O que sÃ£o?

Seeds sÃ£o arquivos CSV usados como tabelas estÃ¡ticas no projeto DBT. Ideal para dados pequenos e constantes como tabelas de referÃªncia, cÃ³digos de UF, calendÃ¡rios etc.

ğŸ” **Como funciona:**
1. Salve um CSV na pasta /seeds/

2. Execute dbt seed

3. O DBT converte o CSV em uma tabela no banco de dados


### Resumo 
```plaintext
macros/
â”œâ”€â”€ utilitarios.sql   # funÃ§Ãµes Jinja para SQL dinÃ¢mico

snapshots/
â”œâ”€â”€ clientes_snapshot.sql   # monitora mudanÃ§as histÃ³ricas

seeds/
â”œâ”€â”€ calendario.csv          # dados estÃ¡ticos
â”œâ”€â”€ tipo_produto.csv

```


# ğŸ—‚ï¸ 3. Estrutura de um Projeto DBT

Um projeto DBT segue uma organizaÃ§Ã£o padronizada de arquivos e pastas, permitindo modularidade, versionamento e manutenÃ§Ã£o clara do pipeline de transformaÃ§Ãµes de dados.

---

## ğŸ“ Estrutura Geral do Projeto

```plaintext
my_dbt_project/
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ packages.yml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ macros/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ snapshots/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ seeds/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ analyses/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ...
â””â”€â”€ target/  
```

ğŸ§¾ dbt_project.yml

Arquivo principal de configuraÃ§Ã£o do projeto. Define:

Nome do projeto

Caminho para modelos

ConfiguraÃ§Ãµes de materializaÃ§Ã£o (ex: view, table)

Versionamento

Pacotes externos

name: my_dbt_project
version: '1.0'
profile: default

models:
  my_dbt_project:
    staging:
      materialized: view
    marts:
      materialized: table


ğŸ”Œ Arquivos de Fonte de Banco de Dados (Entrada e SaÃ­da)
ğŸ“¥ Fonte (entrada): sources

Declarados em arquivos .yml dentro da pasta models/, geralmente sob staging/.

version: 2

sources:
  - name: vendas
    database: raw_db
    schema: ecommerce
    tables:
      - name: pedidos
        description: Tabela de pedidos da loja virtual
        columns:
          - name: id_pedido
            description: Identificador Ãºnico
            tests:
              - not_null
              - unique


Chamado nos modelos SQL via:

SELECT * FROM {{ source('vendas', 'pedidos') }}


ğŸ“¤ Modelos (saÃ­da): ref()
SÃ£o os modelos transformados, referenciados por ref() para manter dependÃªncias claras:
SELECT * FROM {{ ref('stg_pedidos') }}


ğŸ§± Arquivos de Modelo SQL

Localizados dentro de models/, organizados por camada (staging/, intermediate/, marts/, etc.)

Exemplo: models/staging/stg_pedidos.sql

WITH raw AS (
  SELECT * FROM {{ source('vendas', 'pedidos') }}
)

SELECT
  id_pedido,
  cliente_id,
  total::float AS valor_total
FROM raw



ğŸ Arquivos de Modelo Python (dbt v1.3+)

Suporte para transformaÃ§Ã£o com Python estÃ¡ disponÃ­vel em bancos compatÃ­veis (ex: Databricks, Snowpark, etc.). Para DuckDB, esse suporte Ã© ainda limitado ou experimental.

Exemplo: models/intermediate/agg_pedidos.py


def model(dbt, session):
    df = dbt.ref("stg_pedidos")
    df_agg = df.groupby("cliente_id").agg({"valor_total": "sum"})
    return df_agg


ObservaÃ§Ã£o: âš ï¸ Modelos Python exigem engines especÃ­ficos.

ğŸ—’ï¸ Arquivos de DocumentaÃ§Ã£o .yml

Localizados junto aos modelos ou fontes, esses arquivos descrevem cada modelo, coluna, teste e relacionamento.

Exemplo: models/staging/stg_pedidos.yml

version: 2

models:
  - name: stg_pedidos
    description: Modelo staging da tabela de pedidos
    columns:
      - name: id_pedido
        description: Chave primÃ¡ria do pedido
        tests:
          - unique
          - not_null
      - name: valor_total
        description: Total da compra


dbt docs generate
dbt docs serve


âœ… Checklist por zona de dados
Zona	ConteÃºdo	Formato	Arquivos
staging/	Modelos que limpam e padronizam fontes	.sql, .yml	SQL + YAML
intermediate/	Modelos intermediÃ¡rios (joins, lÃ³gica)	.sql, .py	SQL/Python
marts/	Modelos finais para anÃ¡lise (KPIs, etc.)	.sql	SQL
seeds/	Dados estÃ¡ticos em CSV	.csv	CSV + .yml
snapshots/	Controle de mudanÃ§as em dimensÃµes lentas	.sql, .yml	SQL + YAML
macros/	FunÃ§Ãµes reutilizÃ¡veis (em Jinja/SQL)	.sql	SQL (Jinja)


Estrutura completa:

models/
â”œâ”€â”€ staging/
â”‚   â””â”€â”€ stg_tabela.sql / stg_tabela.yml
â”œâ”€â”€ intermediate/
â”‚   â””â”€â”€ join_clientes_pedidos.sql
â”œâ”€â”€ marts/
â”‚   â””â”€â”€ kpi_vendas_mensal.sql
seeds/
â”œâ”€â”€ calendario.csv
â”œâ”€â”€ calendario.yml
snapshots/
â”œâ”€â”€ clientes_snapshot.sql
tests/
â”œâ”€â”€ test_valor_total_positivo.sql
